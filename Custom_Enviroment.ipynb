{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Intro",
   "id": "32a8ba7432811a44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The plan is to have a player blob (blue), which aims to navigate its way as quickly as possible to the food blob (green), while avoiding the enemy blob (red). Now, we could make this super smooth with high definition, but we already know we're going to be breaking it down into observation spaces. Instead, let's just start in a discrete space. Something between a 10x10 and 20x20 should suffice. Do note, the larger you go, the larger your Q-Table will be in terms of space it takes up in memory as well as time it takes for the model to actually learn. So, our environment will be a 20 x 20 grid, where we have 1 player, 1 enemy, and 1 food. For now, we'll just have the player able to move, in attempt to reach the food, which will yield a reward.",
   "id": "3052a274bf96f925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Explanation\n",
    "### 1.Hyperparameters and Constants\n",
    "__Grid and Episodes:__\n",
    "\n",
    "SIZE: Defines the size of the grid environment as 10x10.\n",
    "A 10x10 Q-Table for example, in this case, is ~15MB. A 20x20 is ~195MB\n",
    "\n",
    "HM_EPISODES: The total number of episodes (iterations) for which the agent will be trained.\n",
    "\n",
    "__Rewards and Penalties:__\n",
    "\n",
    "MOVE_PENALTY: The penalty (negative reward) for each move made by the player.\n",
    "ENEMY_PENALTY: The penalty for the player colliding with the enemy.\n",
    "\n",
    "FOOD_REWARD: The reward for the player reaching the food.\n",
    "\n",
    "__Exploration-Exploitation Parameters:__\n",
    "\n",
    "epsilon: Initial probability of choosing a random action (exploration).\n",
    "\n",
    "EPS_DECAY: Factor by which epsilon decays after each episode, reducing exploration over time.\n",
    "\n",
    "__Display Control:__\n",
    "\n",
    "SHOW_EVERY: Controls how often (in terms of episodes) the environment is visually displayed.\n",
    "\n",
    "__Q-Learning Parameters:__\n",
    "\n",
    "start_q_table: A filename to load a pre-trained Q-table or None to start fresh.\n",
    "\n",
    "LEARNING_RATE: Determines how much newly acquired information overrides old information.\n",
    "\n",
    "DISCOUNT: Discount factor for future rewards.\n",
    "\n",
    "__Identifiers and Colors:__\n",
    "\n",
    "PLAYER_N, FOOD_N, ENEMY_N: Numeric identifiers for the player, food, and enemy in the environment.\n",
    "d: A dictionary mapping these identifiers to RGB color values for visualization.\n",
    "\n",
    "### 2. Blob classification\n",
    "Blob Class:\n",
    "\n",
    "Represents an entity (player, food, or enemy) on the grid.\n",
    "\n",
    "Constructor (__init__):\n",
    "\n",
    "Initializes the blob at a random position within the grid.\n",
    "\n",
    "__str__ Method:\n",
    "\n",
    "Returns a string representation of the blob's coordinates, useful for debugging.\n",
    "\n",
    "__sub__ Method:\n",
    "\n",
    "Defines the subtraction operation between two blobs, returning their relative distance as a tuple (dx, dy).\n",
    "\n",
    "action Method:\n",
    "\n",
    "Takes an action (0-3) that moves the blob diagonally in one of four directions.\n",
    "\n",
    "move Method:\n",
    "\n",
    "Moves the blob based on provided x and y values or randomly if not provided.\n",
    "Ensures the blob remains within grid boundaries.\n",
    "### 3. Q_table initialization\n",
    "__Q-Table:__\n",
    "The Q-table is a dictionary that maps observations (states) to a list of Q-values corresponding to each possible action.\n",
    "\n",
    "__Initialization:__\n",
    "If start_q_table is None, the code initializes the Q-table with random values for all possible states.\n",
    "Each state is represented as a tuple of two differences: (player-food, player-enemy), and each entry in the table contains four Q-values, one for each possible action.\n",
    "Loading a Pre-trained Q-Table:\n",
    "If start_q_table is not None, it loads an existing Q-table from a file using pickle.\n",
    "\n",
    "### 4. Main training loop\n",
    "At the start of each episode, the player, food, and enemy are initialized as Blob objects at random positions on the grid.\n",
    "Every SHOW_EVERY episodes, the code sets show to True and prints the current episode number and the average reward for the last SHOW_EVERY episodes.\n",
    "This ensures the environment is visually rendered at intervals, allowing observation of the agent's behavior.\n",
    "\n",
    "### 5. Episode execution\n",
    "__Observations and Actions:__\n",
    "\n",
    "obs: The current state, represented by the relative positions of the player to the food and enemy.\n",
    "The agent selects an action using an epsilon-greedy strategy:\n",
    "With probability epsilon, it takes a random action (exploration).\n",
    "Otherwise, it chooses the action with the highest Q-value for the current state (exploitation).\n",
    "\n",
    "__Action Execution:__\n",
    "\n",
    "The chosen action is executed by calling player.action(action), which moves the player on the grid."
   ],
   "id": "54a6ea90d21ccd1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements",
   "id": "6efa40172bb572cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f64468d7213f64fa"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-09T06:59:43.761856Z",
     "start_time": "2024-08-09T06:59:41.433871Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from PIL import Image  # for creating visual env\n",
    "import cv2  # for showing our visual live\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle  # to save/load Q-Tables\n",
    "from matplotlib import style  # to make pretty charts.\n",
    "import time  # using this to keep track of our saved Q-Tables."
   ],
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
