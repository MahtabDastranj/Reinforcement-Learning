{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Intro",
   "id": "32a8ba7432811a44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The plan is to have a player blob (blue), which aims to navigate its way as quickly as possible to the food blob (green), while avoiding the enemy blob (red). Now, we could make this super smooth with high definition, but we already know we're going to be breaking it down into observation spaces. Instead, let's just start in a discrete space. Something between a 10x10 and 20x20 should suffice. Do note, the larger you go, the larger your Q-Table will be in terms of space it takes up in memory as well as time it takes for the model to actually learn. So, our environment will be a 20 x 20 grid, where we have 1 player, 1 enemy, and 1 food. For now, we'll just have the player able to move, in attempt to reach the food, which will yield a reward.",
   "id": "3052a274bf96f925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Explanation\n",
    "### 1.Hyperparameters and Constants\n",
    "Grid and Episodes:\n",
    "\n",
    "SIZE: Defines the size of the grid environment as 10x10.\n",
    "HM_EPISODES: The total number of episodes (iterations) for which the agent will be trained.\n",
    "Rewards and Penalties:\n",
    "\n",
    "MOVE_PENALTY: The penalty (negative reward) for each move made by the player.\n",
    "ENEMY_PENALTY: The penalty for the player colliding with the enemy.\n",
    "\n",
    "FOOD_REWARD: The reward for the player reaching the food.\n",
    "\n",
    "Exploration-Exploitation Parameters:\n",
    "\n",
    "epsilon: Initial probability of choosing a random action (exploration).\n",
    "\n",
    "EPS_DECAY: Factor by which epsilon decays after each episode, reducing exploration over time.\n",
    "Display Control:\n",
    "\n",
    "SHOW_EVERY: Controls how often (in terms of episodes) the environment is visually displayed.\n",
    "Q-Learning Parameters:\n",
    "\n",
    "start_q_table: A filename to load a pre-trained Q-table or None to start fresh.\n",
    "\n",
    "LEARNING_RATE: Determines how much newly acquired information overrides old information.\n",
    "\n",
    "DISCOUNT: Discount factor for future rewards.\n",
    "Identifiers and Colors:\n",
    "\n",
    "PLAYER_N, FOOD_N, ENEMY_N: Numeric identifiers for the player, food, and enemy in the environment.\n",
    "d: A dictionary mapping these identifiers to RGB color values for visualization.\n",
    "\n",
    "### 2. Blob classification\n",
    "Blob Class:\n",
    "\n",
    "Represents an entity (player, food, or enemy) on the grid.\n",
    "Constructor (__init__):\n",
    "\n",
    "Initializes the blob at a random position within the grid.\n",
    "__str__ Method:\n",
    "\n",
    "Returns a string representation of the blob's coordinates, useful for debugging.\n",
    "__sub__ Method:\n",
    "\n",
    "Defines the subtraction operation between two blobs, returning their relative distance as a tuple (dx, dy).\n",
    "action Method:\n",
    "\n",
    "Takes an action (0-3) that moves the blob diagonally in one of four directions.\n",
    "move Method:\n",
    "\n",
    "Moves the blob based on provided x and y values or randomly if not provided.\n",
    "Ensures the blob remains within grid boundaries.\n",
    "### 3. Q_table initialization\n",
    "Q-Table:\n",
    "The Q-table is a dictionary that maps observations (states) to a list of Q-values corresponding to each possible action.\n",
    "Initialization:\n",
    "If start_q_table is None, the code initializes the Q-table with random values for all possible states.\n",
    "Each state is represented as a tuple of two differences: (player-food, player-enemy), and each entry in the table contains four Q-values, one for each possible action.\n",
    "Loading a Pre-trained Q-Table:\n",
    "If start_q_table is not None, it loads an existing Q-table from a file using pickle.\n",
    "\n",
    "### 4. Main training loop\n",
    "At the start of each episode, the player, food, and enemy are initialized as Blob objects at random positions on the grid.\n",
    "Every SHOW_EVERY episodes, the code sets show to True and prints the current episode number and the average reward for the last SHOW_EVERY episodes.\n",
    "This ensures the environment is visually rendered at intervals, allowing observation of the agent's behavior.\n",
    "\n",
    "### 5. Episode execution\n",
    "Observations and Actions:\n",
    "\n",
    "obs: The current state, represented by the relative positions of the player to the food and enemy.\n",
    "The agent selects an action using an epsilon-greedy strategy:\n",
    "With probability epsilon, it takes a random action (exploration).\n",
    "Otherwise, it chooses the action with the highest Q-value for the current state (exploitation).\n",
    "Action Execution:\n",
    "\n",
    "The chosen action is executed by calling player.action(action), which moves the player on the grid."
   ],
   "id": "54a6ea90d21ccd1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements",
   "id": "6efa40172bb572cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f64468d7213f64fa"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-09T06:59:43.761856Z",
     "start_time": "2024-08-09T06:59:41.433871Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from PIL import Image  # for creating visual env\n",
    "import cv2  # for showing our visual live\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle  # to save/load Q-Tables\n",
    "from matplotlib import style  # to make pretty charts.\n",
    "import time  # using this to keep track of our saved Q-Tables."
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Environment size, constants and variables\n",
    "A 10x10 Q-Table for example, in this case, is ~15MB. A 20x20 is ~195MB"
   ],
   "id": "9d8a903bda363e13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:59:48.157777Z",
     "start_time": "2024-08-09T06:59:48.148625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "style.use('ggplot')\n",
    "SIZE = 10\n",
    "HM_EPISODES = 25000\n",
    "MOVE_PENALTY = 1\n",
    "ENEMY_PENALTY = 300\n",
    "FOOD_REWARD = 25\n",
    "epsilon = 0.9\n",
    "EPS_DECAY = 0.9998\n",
    "SHOW_EVERY = 3000\n",
    "# In case you have a q table, load here (filename)\n",
    "start_q_table = None\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "# key in dict\n",
    "PLAYER_N = 1\n",
    "FOOD_N = 2\n",
    "ENEMY_N = 3\n",
    "# Dict for colors BGR\n",
    "d = {1: (255, 175, 0),\n",
    "     2: (0, 255, 0),\n",
    "     3: (0, 0, 255)}"
   ],
   "id": "969b576190b3c79f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Blob",
   "id": "e1defa8d370b4b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:59:54.474636Z",
     "start_time": "2024-08-09T06:59:54.462010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Blob:\n",
    "    def __init__(self):\n",
    "        self.x = np.random.randint(0, SIZE)\n",
    "        self.y = np.random.randint(0, SIZE)\n",
    "    def __str__(self):\n",
    "        return f'{self.x}, {self.y}'\n",
    "    def __sub__(self, other):\n",
    "        return (self.x - other.x, self.y - other.y)\n",
    "    def action(self, choice):\n",
    "        if choice == 0:\n",
    "           self.move(x=1, y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "    def move(self, x=False, y=False):\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "        \n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > SIZE-1:\n",
    "            self.x = SIZE-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > SIZE-1:\n",
    "            self.y = SIZE-1"
   ],
   "id": "125a2238a0002147",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Q table",
   "id": "7d62b67e7d67971"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if start_q_table is None:\n",
    "    q_table = {}\n",
    "    # (x1, y1), (x2, y2)\n",
    "    for x1 in range(-SIZE+1, SIZE):\n",
    "        for y1 in range(-SIZE+1, SIZE):\n",
    "            for x2 in range(-SIZE+1, SIZE):\n",
    "                for y2 in range(-SIZE+1, SIZE):\n",
    "                    q_table[((x1, y1),(x2,y2))] = [np.random.uniform(-5, 0) for i in range(4)]\n",
    "else:\n",
    "    with open(start_q_table, 'rb') as f:\n",
    "        q_table = pickle.load(f)\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in range(HM_EPISODES):\n",
    "    player = Blob()\n",
    "    food = Blob()\n",
    "    enemy = Blob()\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f'on # {episode}, epsilon: {epsilon}')\n",
    "        print(f\"{SHOW_EVERY} ep mean: {np.mean(episode_rewards[-SHOW_EVERY:])}\")\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    for i in range(200):\n",
    "        obs = (player - food, player - enemy)\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[obs])\n",
    "        else:\n",
    "            action = np.random.randint(0, 4)\n",
    "\n",
    "        player.action(action)\n",
    "        '''\n",
    "        MAYBE\n",
    "        enemy.move()\n",
    "        food.move()\n",
    "        '''\n",
    "        # Rewarding\n",
    "        if player.x == enemy.x and player.y == enemy.y:\n",
    "            reward = -ENEMY_PENALTY\n",
    "        elif player.x == food.x and player.y == food.y:\n",
    "            reward = FOOD_REWARD\n",
    "        else:\n",
    "            reward = -MOVE_PENALTY\n",
    "\n",
    "        # Q values and information\n",
    "        new_obs = (player - food, player - enemy)\n",
    "        max_future_q = np.max(q_table[new_obs])\n",
    "        current_q = q_table[obs][action]\n",
    "\n",
    "        if reward == FOOD_REWARD:\n",
    "            new_q = FOOD_REWARD\n",
    "        elif reward == -ENEMY_PENALTY:\n",
    "            new_q = -ENEMY_PENALTY\n",
    "        else:\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        q_table[obs][action] = new_q\n",
    "\n",
    "        episode_rewards += reward\n",
    "\n",
    "        # Displaying the environment\n",
    "        if show:\n",
    "            env = np.zeros((SIZE, SIZE, 3), dtype=np.uint8)\n",
    "            env[food.x][food.y] = d[FOOD_N]\n",
    "            env[player.x][player.y] = d[PLAYER_N]\n",
    "            env[enemy.x][enemy.y] = d[ENEMY_N]\n",
    "\n",
    "            img = Image.fromarray(env, 'RGB')\n",
    "            img = img.resize((300, 300))\n",
    "            cv2.imshow('', np.array(img))\n",
    "\n",
    "            # Handling rewards\n",
    "            if reward == FOOD_REWARD or reward == -ENEMY_PENALTY:\n",
    "                if cv2.waitKey(500) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon *= EPS_DECAY"
   ],
   "id": "76c54768d04b930b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graphs and savings\n",
   "id": "a30507052899c853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,)) / SHOW_EVERY, mode='valid')\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.ylabel(f\"Reward {SHOW_EVERY}ma\")\n",
    "plt.xlabel(\"episode #\")\n",
    "plt.show()\n",
    "\n",
    "with open(f\"qtable-{int(time.time())}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(q_table, f)"
   ],
   "id": "c919277fcd5fe8bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
